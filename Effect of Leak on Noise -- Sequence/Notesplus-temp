Using the equation:
$$
\tau \frac{du}{dt} = -Lu + I + \alpha W_1
$$
Let simplify to:
$$
\tau \frac{du}{dt} = -Lu + B
$$
Using the method of Euler (as it is what we used):
$$
u(t) = u(t-1) + \frac{1}{\tau}(-Lu(t-1) + B(t-1))\\
u(t) = \Big(1-\frac{L}{\tau}\Big)u(t-1) + \frac{1}{\tau}B(t-1)
$$
Developping $u(t-1)$ further:
$$
u(t) = \Big(1-\frac{L}{\tau}\Big)
\Bigg[ \Big(1-\frac{L}{\tau}\Big)u(t-2) + \frac{1}{\tau}B(t-2)\Bigg]
+ \frac{1}{\tau}B(t-1)
$$
And $u(t-2)$ further:
$$
u(t) = \Big(1-\frac{L}{\tau}\Big)
\Bigg[ \Big(1-\frac{L}{\tau}\Big)
\Bigg[ \Big(1-\frac{L}{\tau}\Big)u(t-3) + \frac{1}{\tau}B(t-3)\Bigg]
+ \frac{1}{\tau}B(t-2)\Bigg]
+ \frac{1}{\tau}B(t-1)
$$
We obtain:
$$
\Big(1-\frac{L}{\tau}\Big)^3u(t-3) + \frac{1}{\tau}B(t-1)
+ \frac{1}{\tau}\Big(1-\frac{L}{\tau}\Big)B(t-2) +\frac{1}{\tau}\Big(1-\frac{L}{\tau}\Big)^2B(t-3)
$$
That can be written:
$$
\Big(1-\frac{L}{\tau}\Big)^3u(t-3) + \frac{1}{\tau}\Big(1-\frac{L}{\tau}\Big)^0B(t-1)
+ \frac{1}{\tau}\Big(1-\frac{L}{\tau}\Big)^1B(t-2) +\frac{1}{\tau}\Big(1-\frac{L}{\tau}\Big)^2B(t-3)
$$
Thus, we can see the following pattern:
$$
u(t) = \Big(1-\frac{L}{\tau}\Big)^Nu(t-N) + \frac{1}{\tau}\sum_{s=1}^{N}\Big(1-\frac{L}{\tau}\Big)^{s-1}B(t-s)
$$

if $N=t-t_0$ and $t_0=0$, we can write:
$$
u(t) = \Big(1-\frac{L}{\tau}\Big)^t u(0)+ \frac{1}{\tau}\sum_{s=1}^{t}\Big(1-\frac{L}{\tau}\Big)^{s-1}B(t-s)
$$
# continuous counter part:

The equation we are using is similar to the integrate and fire neuron model for which the continuous solution is well known:
$$
u(t) = \exp\Big(-\frac{L}{\tau}t\Big)u(0) + \frac{1}{\tau} \int_{0}^{t}\exp\Big(-\frac{L}{\tau}s\Big)B(t-s)ds
$$

Is the function $\Big(1-\frac{L}{\tau}\Big)^t$ equivalent to $\exp\Big(-\frac{L}{\tau}t\Big)$ ?
``` {r}
almostExp <- function(L, tau, t){
  return ((1- L/tau)^t)
}

Exp <- function(L, tau, t){
  return (exp(-L*t/tau))
}

time <- 0:1000

plot(time, Exp(10, 1000, time))
points(time, almostExp(10, 1000, time), col='red')

plot(time, Exp(100, 100, time))
points(time, almostExp(100, 100, time), col='red')
```

Importantly note that when $\tau = L$, then $\Big(1-\frac{L}{\tau}\Big)^t = 1$ for $t=0$

### Now what happen for different value of L and \tau ?
Setting $\tau > 1$ and $u(0) = 0$,

- If $L = \tau$:
  $$
  u(t) = \frac{1}{\tau}\sum_{s=1}^{t}\Big(0\Big)^{s-1}B(t-s)
  $$
  Remembering that $0^0 = 1$, we obtain:
  $$
  u(t) = \frac{1}{\tau}B(t-s)
  $$
  The value of u(t) is a perfect/instantaneous/exact copy of the input (or noise), scaled by $1/\tau$.

- If $L = 0$:
  $$
  u(t) = \frac{1}{\tau}\sum_{s=1}^{t}B(t-s)
  $$
  That is a perfect accumulation, scaled by $1/\tau$.

- If $L<\tau$:
  $$
  u(t)= \frac{1}{\tau}\sum_{s=1}^{t}\Big(1-\frac{L}{\tau}\Big)^{s-1}B(t-s)\\
  $$
  is converging because:
  $$
  \lim_{t\to\infty}  \Big(1-\frac{L}{\tau}\Big)^{s-1} = 0
  $$
  this also means that, although u(t) accumulate recent input, it forgets them with time (smaller weight   with time).
  **Thus, its maximum accumulated variance is bounded.**
